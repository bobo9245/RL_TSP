{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network(QNet) 사용 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import 및 파라미터 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from network import QTrainer, QNet\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "SEED = 1                     # A seed for the random number generator\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Graph\n",
    "NR_NODES = 998               # Number of nodes N\n",
    "EMBEDDING_DIMENSIONS = 5     # Embedding dimension D\n",
    "EMBEDDING_ITERATIONS_T = 1   # Number of embedding iterations T\n",
    "\n",
    "# Learning\n",
    "NR_EPISODES = 100\n",
    "MEMORY_CAPACITY = 10000\n",
    "N_STEP_QL = 2                # Number of steps (n) in n-step Q-learning to wait before computing target reward estimate\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "GAMMA = 0.9\n",
    "INIT_LR = 5e-3\n",
    "LR_DECAY_RATE = 1. - 2e-5    # learning rate decay\n",
    "\n",
    "MIN_EPSILON = 0.1\n",
    "EPSILON_DECAY_RATE = 6e-4    # epsilon decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State, action 관련 자료형, 함수, 클래스 정의\n",
    "- State : 현재 state에 대한 정보를 저장하기 위한 자료형 \n",
    "- Experience : state tensor들을 한번만 계산하기 위해 experience 인스턴스에 저장합니다.  \n",
    "- state2tens : state를 5개의 차원으로 embedding하는 함수  \n",
    "- 여러 experience를 저장해두기 위한 메모리 클래스  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ('W', 'coords', 'partial_solution'))\n",
    "Experience = namedtuple('Experience', ('state', 'state_tsr', 'action', 'reward', 'next_state', 'next_state_tsr'))\n",
    "\n",
    "\n",
    "def state2tens(state):\n",
    "    solution = set(state.partial_solution)\n",
    "    sol_last_node = state.partial_solution[-1] if len(state.partial_solution) > 0 else -1\n",
    "    sol_first_node = state.partial_solution[0] if len(state.partial_solution) > 0 else -1\n",
    "    coords = state.coords\n",
    "    nr_nodes = coords.shape[0]\n",
    "\n",
    "    xv = [[(1 if i in solution else 0),           # 해당 노드를 방문 했는지 여부\n",
    "           (1 if i == sol_first_node else 0),     # 해당 노드가 시작 노드인지 여부 \n",
    "           (1 if i == sol_last_node else 0),      # 해당 노드가 마지막 노드인지 여부\n",
    "           coords[i,0],                           # 해당 노드의 x좌표\n",
    "           coords[i,1]                            # 해당 노드의 y좌표\n",
    "          ] for i in range(nr_nodes)]\n",
    "    \n",
    "    return torch.tensor(xv, dtype=torch.float32, requires_grad=False, device=device)\n",
    "\n",
    "\n",
    "class Memory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        self.nr_inserts = 0\n",
    "        \n",
    "    def remember(self, experience):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = experience\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.nr_inserts += 1\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.nr_inserts, self.capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그 외의 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_distance(solution, W):\n",
    "    if len(solution) < 2:\n",
    "        return 0 \n",
    "    \n",
    "    total_dist = 0\n",
    "    for i in range(len(solution) - 1):\n",
    "        total_dist += W[solution[i], solution[i+1]].item()\n",
    "        \n",
    "    if len(solution) == W.shape[0]:\n",
    "        total_dist += W[solution[-1], solution[0]].item()\n",
    "\n",
    "    return total_dist\n",
    "\n",
    "        \n",
    "def is_state_final(state):\n",
    "    return len(set(state.partial_solution)) == state.W.shape[0]\n",
    "\n",
    "\n",
    "def get_next_neighbor_random(state):\n",
    "    solution, W = state.partial_solution, state.W\n",
    "    \n",
    "    if len(solution) == 0:\n",
    "        return random.choice(range(W.shape[0]))\n",
    "    already_in = set(solution)\n",
    "    candidates = list(filter(lambda n: n.item() not in already_in, W[solution[-1]].nonzero()))\n",
    "    if len(candidates) == 0:\n",
    "        return None\n",
    "    return random.choice(candidates).item()\n",
    "\n",
    "\n",
    "def get_distance_matrix(x, num_cities=998):\n",
    "    x = torch.tensor(x)\n",
    "    x1, x2 = x[:,0:1], x[:,1:2]\n",
    "    d1 = x1 - (x1.T).repeat(num_cities,1)\n",
    "    d2 = x2 - (x2.T).repeat(num_cities,1)\n",
    "    distance_matrix = (d1**2 + d2**2)**0.5   # Euclidean Distance\n",
    "    return distance_matrix.numpy()\n",
    "\n",
    "\n",
    "def init_model(fname=None):\n",
    "    Q_net = QNet(EMBEDDING_DIMENSIONS, T=EMBEDDING_ITERATIONS_T).to(device)\n",
    "    optimizer = optim.Adam(Q_net.parameters(), lr=INIT_LR)\n",
    "    lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=LR_DECAY_RATE)\n",
    "    \n",
    "    if fname is not None:\n",
    "        checkpoint = torch.load(fname)\n",
    "        Q_net.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    \n",
    "    Q_trainer = QTrainer(Q_net, optimizer, lr_scheduler)\n",
    "    return Q_trainer, Q_net, optimizer, lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m W_np \u001b[38;5;241m=\u001b[39m get_distance_matrix(coords)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# init Trainer, Model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m Q_trainer, Q_net, optimizer, lr_scheduler \u001b[38;5;241m=\u001b[39m \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# generate memory\u001b[39;00m\n\u001b[1;32m     11\u001b[0m memory \u001b[38;5;241m=\u001b[39m Memory(MEMORY_CAPACITY)\n",
      "Cell \u001b[0;32mIn[3], line 42\u001b[0m, in \u001b[0;36minit_model\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_model\u001b[39m(fname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     41\u001b[0m     Q_net \u001b[38;5;241m=\u001b[39m QNet(EMBEDDING_DIMENSIONS, T\u001b[38;5;241m=\u001b[39mEMBEDDING_ITERATIONS_T)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 42\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mINIT_LR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mExponentialLR(optimizer, gamma\u001b[38;5;241m=\u001b[39mLR_DECAY_RATE)\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/optim/adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/optim/optimizer.py:278\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m--> 278\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_compile.py:22\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m allowed_functions, convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcode_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m code_context\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/convert_frame.py:62\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mguards\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     57\u001b[0m     CheckFunctionManager,\n\u001b[1;32m     58\u001b[0m     get_and_maybe_log_recompilation_reason,\n\u001b[1;32m     59\u001b[0m     GuardedCode,\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Hooks\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutput_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OutputGraph\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_record\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExecutionRecord\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msymbolic_convert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InstructionTranslator, SpeculationLog\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/output_graph.py:39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sympy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythonReferenceAnalysis\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mweak\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config, logging \u001b[38;5;28;01mas\u001b[39;00m torchdynamo_logging, variables\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompiledFn, CompilerFn\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbytecode_transformation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     42\u001b[0m     create_call_function,\n\u001b[1;32m     43\u001b[0m     create_instruction,\n\u001b[1;32m     44\u001b[0m     Instruction,\n\u001b[1;32m     45\u001b[0m     unique_id,\n\u001b[1;32m     46\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/variables/__init__.py:68\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NNModuleVariable, UnspecializedNNModuleVariable\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     62\u001b[0m     FakeItemVariable,\n\u001b[1;32m     63\u001b[0m     NumpyNdarrayVariable,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     UnspecializedPythonVariable,\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     69\u001b[0m     TorchCtxManagerClassVariable,\n\u001b[1;32m     70\u001b[0m     TorchInGraphFunctionVariable,\n\u001b[1;32m     71\u001b[0m     TorchVariable,\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_defined\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserDefinedClassVariable, UserDefinedObjectVariable\n\u001b[1;32m     75\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionContextVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutogradFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWithExitFunctionVariable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    129\u001b[0m ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/py39/lib/python3.9/site-packages/torch/_dynamo/variables/torch.py:95\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     80\u001b[0m     constant_fold_functions\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m     81\u001b[0m         [\n\u001b[1;32m     82\u001b[0m             torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mis_initialized,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m         ]\n\u001b[1;32m     86\u001b[0m     )\n\u001b[1;32m     89\u001b[0m tracing_state_functions \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m     torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39mis_fx_tracing: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     94\u001b[0m     torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mis_in_onnx_export: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m---> 95\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexternal_utils\u001b[49m\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     96\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39mis_compiling: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     97\u001b[0m }\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseTorchVariable\u001b[39;00m(VariableTracker):\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"common base for all torch.* functions, classes, modules and other things\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "# TSP Data Load\n",
    "coords = np.array(pd.read_csv('2024_AI_TSP.csv', header=None))\n",
    "\n",
    "# make distance matrix\n",
    "W_np = get_distance_matrix(coords)\n",
    "\n",
    "# init Trainer, Model\n",
    "Q_trainer, Q_net, optimizer, lr_scheduler = init_model()\n",
    "\n",
    "# generate memory\n",
    "memory = Memory(MEMORY_CAPACITY)\n",
    "\n",
    "\n",
    "losses = []\n",
    "path_lengths = []\n",
    "found_solutions = dict()\n",
    "current_min_med_length = float('inf')\n",
    "\n",
    "\n",
    "for episode in range(NR_EPISODES):\n",
    "    \n",
    "    # tensor (distance matrix)\n",
    "    W = torch.tensor(W_np, dtype=torch.float32, requires_grad=False, device=device)\n",
    "    \n",
    "    # start node = 0\n",
    "    solution = [0]\n",
    "    \n",
    "    # current state\n",
    "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "    current_state_tsr = state2tens(current_state)\n",
    "    \n",
    "\n",
    "    # define state, state_tsrs(embedding), reward, action list\n",
    "    states = [current_state]\n",
    "    states_tsrs = [current_state_tsr] \n",
    "    rewards = []\n",
    "    actions = []\n",
    "    \n",
    "    \n",
    "    # current value of epsilon\n",
    "    epsilon = max(MIN_EPSILON, (1-EPSILON_DECAY_RATE)**episode)\n",
    "    \n",
    "\n",
    "    while not is_state_final(current_state):\n",
    "        \n",
    "        # select next node\n",
    "        if epsilon >= random.random():\n",
    "            next_node = get_next_neighbor_random(current_state)\n",
    "        else:\n",
    "            next_node, est_reward = Q_trainer.get_best_action(current_state_tsr, current_state)\n",
    "        \n",
    "\n",
    "        # append next node to solution\n",
    "        next_solution = solution + [next_node]\n",
    "\n",
    "        # calulate reward\n",
    "        reward = -(total_distance(next_solution, W) - total_distance(solution, W))\n",
    "        \n",
    "        \n",
    "        next_state = State(partial_solution=next_solution, W=W, coords=coords)\n",
    "        next_state_tsr = state2tens(next_state)\n",
    "        \n",
    "        states.append(next_state)\n",
    "        states_tsrs.append(next_state_tsr)\n",
    "        rewards.append(reward)\n",
    "        actions.append(next_node)\n",
    "        \n",
    "        \n",
    "        if len(solution) >= N_STEP_QL:\n",
    "            memory.remember(Experience(state=states[-N_STEP_QL],\n",
    "                                       state_tsr=states_tsrs[-N_STEP_QL],\n",
    "                                       action=actions[-N_STEP_QL],\n",
    "                                       reward=sum(rewards[-N_STEP_QL:]),\n",
    "                                       next_state=next_state,\n",
    "                                       next_state_tsr=next_state_tsr))\n",
    "            \n",
    "        if is_state_final(next_state):\n",
    "            for n in range(1, N_STEP_QL):\n",
    "                memory.remember(Experience(state=states[-n],\n",
    "                                           state_tsr=states_tsrs[-n], \n",
    "                                           action=actions[-n], \n",
    "                                           reward=sum(rewards[-n:]), \n",
    "                                           next_state=next_state,\n",
    "                                           next_state_tsr=next_state_tsr))\n",
    "        \n",
    "        \n",
    "        current_state = next_state\n",
    "        current_state_tsr = next_state_tsr\n",
    "        solution = next_solution\n",
    "        \n",
    "\n",
    "        loss = None\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "\n",
    "            # sampling batch experience\n",
    "            experiences = memory.sample_batch(BATCH_SIZE)\n",
    "            \n",
    "            batch_states_tsrs = [e.state_tsr for e in experiences]\n",
    "            batch_Ws = [e.state.W for e in experiences]\n",
    "            batch_actions = [e.action for e in experiences]\n",
    "            batch_targets = []\n",
    "            \n",
    "\n",
    "            for i, experience in enumerate(experiences):\n",
    "                target = experience.reward\n",
    "                if not is_state_final(experience.next_state):\n",
    "                    #이게 중요\n",
    "                    _, best_q_value = Q_trainer.get_best_action(experience.next_state_tsr, experience.next_state)\n",
    "                    target += GAMMA * best_q_value\n",
    "                batch_targets.append(target)\n",
    "            # 이걸 기반으로 하여 Experiences 학습시킨다. batch 단위로 loss 연산을 하고 학습시킨다.\n",
    "            loss = Q_trainer.batch_update(batch_states_tsrs, batch_Ws, batch_actions, batch_targets)\n",
    "            losses.append(loss)\n",
    "\n",
    "    length = total_distance(solution, W)\n",
    "    path_lengths.append(length)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print('Ep %d. Loss = %.3f, length = %.3f, epsilon = %.4f, lr = %.4f' % (\n",
    "            episode, (-1 if loss is None else loss), length, epsilon,\n",
    "            Q_trainer.optimizer.param_groups[0]['lr']))\n",
    "        found_solutions[episode] = (W.clone(), coords.copy(), [n for n in solution])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [0]\n",
    "current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "current_state_tsr = state2tens(current_state)\n",
    "\n",
    "while not is_state_final(current_state):\n",
    "    next_node, est_reward = Q_trainer.get_best_action(current_state_tsr, \n",
    "                                                    current_state)\n",
    "    \n",
    "    solution = solution + [next_node]\n",
    "    current_state = State(partial_solution=solution, W=W, coords=coords)\n",
    "    current_state_tsr = state2tens(current_state)\n",
    "    \n",
    "print(\"Final solution : \", str(solution))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
